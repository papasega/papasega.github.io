---
layout: post
title: La reconnaissance vocale en Wolof
subtitle: Comment √ßa marche ?
bigimg: img/Websites_Redesign_Backgrounds_DT_Robotics1_2018.jpg
#share-img: https://github.com/papasega/psw_blog/tree/master/img/posts/ARS.png
tags: [ASR, Speech Recognition, NLP]
comments: true
categories: deep-learning
---



Reconnaissance automatique de la parole en Wolof
============

![image](https://drive.google.com/uc?export=view&id=1XxfOCSBtsxSFxjot5dFAA0YFYjZgFOxT)


Introduction 
============

Les technologies d'ing√©nierie linguistique font fonctionner de nombreuses applications au sein des moteurs de recherche, des chatbots, forums, etc. Pour traiter le langage et la parole, ces technologies combinent des domaines tels que la syntaxe ou la s√©mantique, laquelle constitue un champ de recherche en progr√®s continu pour offrir une compr√©hension toujours plus affin√©e du sens. 

Nous allons parler, dans cet article, du syst√®me de reconnaissance automatique de la parole des **langues d'Afrique subsaharienne**. 
En particulier le **wolof**, une langue parl√©e par plus de **90%** de la population s√©n√©galaise d'apr√®s l'ANSD.


On se pose la question de savoir, mais comment fonctionne r√©ellement un syt√®me de Reconnaissance Automatique de la Parole (RAP) ou Automatic Speech Recognition (ASR) en anglais ? 
![image](https://drive.google.com/uc?export=view&id=1pwP0gfrYmjZRQukvrDpWvKY4FrA1h_Ea)
Nous verrons la r√©ponse √† cette question dans la suite de cet article.  

 **********************************************************************************************************************
 **********************************************************************************************************************
 **********************************************************************************************************************

# Syst√®me de reconnaissance automatique de la parole


  
## 1. D√©finition et Objectifs  
La reconnaissance automatique de la parole (RAP) ou Automatic Speech Recognition (ASR) en anglais est d√©finie comme √©tant une technique qui vise √† faire reconna√Ætre par un ordinateur les mots ou les phrases prononc√©es par une ou plusieurs personnes. 

Un tel syst√®me analyse ainsi un Ô¨Çux audio et transcrit, le plus Ô¨Åd√®lement possible, les paroles prononc√©es en texte. 
C‚Äôest donc une technique informatique qui permet d‚Äôanalyser la voix humaine capt√©e au moyen d‚Äôun microphone pour la transcrire sous la forme d‚Äôun texte exploitable par une machine.


**La reconnaissance de la parole** a pour objectif d‚Äôextraire une information lexicale (mots, suite de mots ou hypoth√®ses de mots) √† partir d‚Äôune information acoustique (le signal de parole). 
Cependant la **compr√©hension de la parole** (pour des applications de dialogue naturel que nous allons pas traiter ici) essaye d‚Äôextraire en plus une information s√©mantique qui permet d‚Äôavoir une connaissance des intentions de l‚Äôutilisateur.


Dans la suite de cet article, je vais vous parler, certaines de mes exp√©riences acad√©miques dans le  domaine de la parole et de l'audio.
En effet il s'agira en particulier de la reconnaissance vocale dans une langue d'Afrique subsaharienne: le **Wolof**.

### 1.1  Pourquoi la reconnaissance vocale ? 

Avant de commencer √† d√©velopper le sujet, parlons de la noble cause de cette technologie qui lutte contre les in√©galit√©s num√©riques. En effet, imaginons un **p√™cheur** qui veut aller sur mer et qui ne sait pas utiliser son smartphone pour regarder les conditions m√©t√©orologiques.  Si toutefois il pouvait recevoir ces informations sur son terminal en sa **langue locale** cela lui facilitera drastiquement sa vie et son activ√©. De m√™me pour un **agriculteur** du monde rural qui veut avoir des informations sur les semences et la fertilit√© des sols en sa langue locale et en **vocale**.

### 1.2  Parlons un peu du S√©n√©gal. 

Le S√©n√©gal √©tant un pays riche de par sa diversit√© √©thnique avec une multitude de langues locales (plus 30 environs) dont le **wolof** qui est parl√© par plus de **90%** de la population bien que le fran√ßais soit la langue officielle, n'est compris que moins de **25%**.

Comme les **60 %**  de cette population ne savent ni lire, ni √©crire. De ce fait une infime partie de la population est connect√©e et b√©n√©ficie de toutes les commodit√©s du num√©rique. 

Ainsi l'utilisation de **l'intelligence artificielle** peut relever ce d√©fi tout en permettant √† tout un chacun de communiquer √†  temps avec sa langue natale.

 

## 2. Pourquoi la parole ?

Tout simplement parce que la parole et l'audio sont tr√®s pertinents dans les tendances li√©es √† **l'intelligence artificielle**. La premi√®re tendance √©tant la voix. Ces derni√®res ann√©es, nous avons vu une tendance remarquable de personnes s'engageant avec des haut-parleurs intelligents pour √™tre assist√©es sur divers produits, logiciels et mat√©riels diff√©rents en utilisant uniquement leur voix, c'est incroyable ! Que ce soit pour obtenir des informations sur ce qui se passe dans leur journ√©e , appeler vos amis et votre famille, etc.

La voix est une interface tr√®s naturelle que les gens trouvent tr√®s utile et facile √† utiliser et l'autre tendance que nous avons remarqu√©e est l'audio.
Si vous √™tes tenu au courant de toute forme de nouvelles, vous verrez que le contenu audio et vid√©o est en constante augmentation dans la production. Et la question est de savoir comment nous pouvons aider **les personnes r√©put√©es d'illettrisme** √† vivre une exp√©rience formidable dans ce monde du num√©rique. Aussi comment les aidons-nous √† **avoir le pouvoir** de g√©rer des services, sur leurs activit√©s √©conomiques, autour de la parole et de l'audio dans leur langue maternelle ou locale.

### 2.1  L'importance de la parole et de l'audio 

La premi√®re chose √† laquelle nous pensons est de savoir comment utiliser la voix pour aider les gens √† dialoguer avec **un robot en vocal** et √† vivre une **exp√©rience naturelle, facile et l√©g√®re**. La parole et l'audio sont vraiment importants, et √† long terme nous devons continuer √† y investir pour r√©ussir le d√©fi de lutte contre l'in√©galit√© num√©rique. L'Afrique compte **plus de 2000 langues** tr√®s li√©es aux ethnies et peu li√©es aux nations. Ainsi l‚Äôexpression linguistique est donc n√©cessaire. Il est important que nous ayons une grande exp√©rience dans les diff√©rentes langues d'Afrique Subsaharienne. La mise √† l'√©chelle pour prendre en charge cela n'est pas n√©cessairement facile. La cr√©ation de mod√®les d'apprentissage automatique comporte de nombreux d√©fis diff√©rents. 

## 3.  Le syst√®me de reconnaissance vocale  

![image](https://drive.google.com/uc?export=view&id=1XxfOCSBtsxSFxjot5dFAA0YFYjZgFOxT)


### 3.1 Le mod√®le acoustique
Le mod√®le acoustique √©tant le mod√®le utilis√© dans la reconnaissance automatique de la parole pour repr√©senter la relation entre un signal audio et les **phon√®mes** ou autres entit√©s linguistiques qui composent la parole. Ce mod√®le est tir√© d'un ensemble d'enregistrement audio et de leurs transcriptions correspondantes (avec un logiciel: **Transcriber**). Il est cr√©√© en prenant des enregistrements audio de la parole et de leurs transcriptions de texte, et en utilisant un logiciel pour cr√©er des repr√©sentations statistiques des sons qui composent chaque mot. Cela peut √™tre pour chaque langue diff√©rente, tout le vocabulaire, les mod√®les de discours, l'argot local etc.
Passons √† certains de ces d√©fis √† un tr√®s bref aper√ßu sur la figure ci-dessous.

![image](https://drive.google.com/uc?export=view&id=1TtPTYB5ft89yo_OGPBtsviwF94JGuSHo)

**Comment la reconnaissance vocale fonctionne √† un niveau avanc√© ?** 

Le but des t√¢ches de la reconnaissance vocale est de prendre un fichier audio et de faire un mapping avec un fichier texte qui est une pr√©diction de ce qui a √©t√© prononc√© dans le signal audio. Pour cela il faut des fonctionnalit√©s, pas toutes faciles √† appr√©hender. Nous pouvons le soumettre √† diff√©rents mod√®les entra√Æn√©s sur des donn√©es, puis faire une pr√©diction de ce qui a √©t√© dite. 
Passons √† un exemple explicite dans la section suivante.

### 3.2  La phonologie d'une langue et mod√®le acoustique de l'ASR.

![image](https://drive.google.com/uc?export=view&id=1pkUeNuDGJ3ID3n1dgdRexMYEnpzuN1xa)

Le travail du mod√®le acoustique consiste √† prendre des **phon√®mes audio** et de sortir des unit√©s sonores qui repr√©sentent une sorte de **phonologie de la langue**, toutes les diff√©rentes unit√©s de son qui composent les mots.
Vous pourriez bien dire mais comment donner un sens √† toutes ces diff√©rentes unit√©s de sons ?
Heureusement, nous avons un **dictionnaire** qui repr√©sente le vocabulaire en quelque sorte **phon√©tis√©**. Il s'agit d'une correspondance entre les mots et la fa√ßon dont vous les prononcez dans les unit√©s sonores. Ce dictionnaire est ce que nous utilisons pour donner un sens √† tout cela. Les diff√©rents phon√®mes de notre mod√®le acoustique en passant aux mots sensibles. Mais maintenant, nous avons un tas de mots qui doivent √™tre misent dans une phrase ou plusieurs phrases. En r√©alit√© nous avons 10 mots dans notre exemple ci-dessous.

![image](https://drive.google.com/uc?export=view&id=18CCGCCdAiT8KpxkgNHbvSd3V9QTQVDnb)

Nous pouvons utiliser de nombreuses combinaisons diff√©rentes pour donner une phrase √† ces diff√©rents mots. La question est alors comment d√©terminez-vous quelle est **la bonne combinaison ?** C'est l√† que vous utiliserez un **mod√®le de langage** qui vous aidera √† comprendre le contexte d'une phrase sur les mots et √† d√©terminer la combinaison la plus probable, la s√©quence de ces mots qui ont du sens. Ainsi nous avons l√† notre sortie du ASR c'est √† dire la transcription du signal d'entr√©. 


### 3.3  Le mod√®le de langage

Le r√¥le du **mod√®le de langage** est d‚Äôestimer la probabilit√© √† priori de s√©quence de mot <img src="https://latex.codecogs.com/svg.latex?\Large&space; P(W) " title="  " />.  Il existe dans les faits deux fa√ßons de proc√©der pour cr√©er un mod√®le de langage. Il s‚Äôagit soit de d√©finir un syst√®me utilisant une grammaire formelle d√©finie par des linguistes (approche dans les faits trop compliqu√©e √† mettre en place) soit d‚Äôutiliser un mod√®le statistique appris automatiquement √† partir de vastes corpus de textes.
Les mod√®les statistiques du langage sont la plupart du temps des mod√®les N-grammes, car ces derniers ont l‚Äôavantage d‚Äô√™tre performants et simples.

**Exemple:**

             Na ngen deef 
             Dala ak j√†mm si Reewu Terannga bi 
             =================================
             Comment allez-vous ? 
             Bienvenu dans le pays de la Terannga
             
![image](https://drive.google.com/uc?export=view&id=1CAZlm7wwfTeYKPgQtNVEg2IKympUpX-u)

Maintenant, si vous souhaitez cr√©er cette combinaison dans toutes les langues. Vous allez faire la m√™me t√¢che pour ces trois composants et les construire pour chaque langue que vous souhaitez prendre en charge, ce qui devient un peu complexe, comme nous le verrons dans la suite ( section mod√©lisation math√©matiques).
Alors, quel est exactement le grand d√©fi de la mise √† l'√©chelle vers toutes ces diff√©rentes langues ? 

### 3.3  La probl√©matique des donn√©es

Toute la difficult√© de ce domaine repose sur les ressources linguistiques et surtout quand ces derniers sont r√©put√©s d'une **rarissime** par excellence. Comme le cas du wolof qui est une langue peu dot√©e et v√©hiculaire.
Il y a √©videmment un composant de collecte de donn√©es dont nous avons besoin pour former nos mod√®les acoustiques et nos mod√®les de langage. Mais lorsque nous avons un dictionnaire de prononciation ou nous devons avoir une connaissance phonologique linguistique approfondie d'une langue, c'est ce genre de chose que les gens vont √† l'√©cole pour √©tudier pendant des ann√©es dans une langue donn√©e. C'est l√† que √ßa devient difficile voir m√™me tr√®s difficile √† r√©aliser ce genre de syst√®me. Vous pourriez vous dire, pourquoi ne pas trouver tous les experts linguistes ?

Eh bien, imaginez qu'il n'y a que 7 millions de personnes qui parlent une langue donn√©e dans le monde et que sur ces 7 millions, nous devons trouver les quelques experts qui existent connaissant r√©ellement toutes les nuances de cette langue. Vous pouvez voir √† quel point cela devient difficile √† mesure que vous √©voluez vers de plus en plus de langues. **C'est un peu le cas en Afrique o√π nous avons 2000 langues mais heureusement seulement 40 langues peuvent couvrir le territoire africain**. 

N'oubliez pas non plus que ce n'est pas un langage de t√¢che unique. La langue n‚Äôest pas statique. La langue √©volue au fil du temps, de nouveaux mots apparaissent, les anciens mots s‚Äô√©vanouissent, les changements de sens, les prononciations changent, la langue vernaculaire et l‚Äôargot apparaissent, il s‚Äôagit donc d‚Äôun probl√®me permanent auquel vous devez faire face. Alors, comment pouvons-nous mettre √† l'√©chelle la fabrication artisanale de ces dictionnaires. C‚Äôest assez difficile.

Eh bien, prenons un peu de recul et r√©-√©valuons simplement ce que nous faisons de bout en bout dans le processus ASR. Y a-t-il une meilleure fa√ßon d'avoir des donn√©es audio, de les faire passer par un mod√®le acoustique afin d'obtenir un tas de phon√®mes. Nous prenons ces phon√®mes pour obtenir des mots √† partir d'un dictionnaire, puis √† partir des mots, nous obtenons une phrase. Le gros probl√®me que nous avons est la grande d√©pendance aux syst√®mes phon√©tiques qui est le coeur du syst√®me de reconnaissance automatique de la parole.

![image](https://drive.google.com/uc?export=view&id=1pwP0gfrYmjZRQukvrDpWvKY4FrA1h_Ea)

## 4. Mod√©lisation math√©matiques:

Nous allons essayer, dans cette partie, de mod√©liser cette figure ci-dessous, qui est la vue sh√©matique d'un syst√®me de reconnaissance automatique de la parole, en √©quations math√®matiques et comprendre comment fonction le mod√®le de langage. 

![image](https://drive.google.com/uc?export=view&id=1XxfOCSBtsxSFxjot5dFAA0YFYjZgFOxT)

### 4.1 Principes g√©n√©raux 
 
Le but d'un syst√®me de reconnaissance automatique de la parole est de fournir la
transcription textuelle d'un signal audio d'entr√©e contenant de la parole. Dans le cadre de la
mod√©lisation statistique de la parole, cette t√¢che √©quivaut √† rechercher parmi l'ensemble des
s√©quences de mots possibles √† partir d'un vocabulaire fix√© la s√©quence la plus probable <img src="https://latex.codecogs.com/svg.latex?\Large&space; W^* " title="  " />
√©tant donn√©e une s√©quence <img src="https://latex.codecogs.com/svg.latex?\Large&space; A " title="  " />  de caract√©ristiques acoustiques observ√©es √† partir du signal
d'entr√©e. Math√©matiquement, cela s'√©crit sous la forme de la maximisation a posteriori
suivante :

![image](https://drive.google.com/uc?export=view&id=1Zd0kjX86n4DNlaBXunMn75AEjytgzLJm)

<img src="https://latex.codecogs.com/svg.latex?\Large&space; p(A|W) " title="  " /> est la vraisemblance des observations acoustiques sachant une s√©quence de mots test√©e,  <img src="https://latex.codecogs.com/svg.latex?\Large&space; P(W) " title="  " /> estv la probabilit√© a priori de cette s√©quence de mots et <img src="https://latex.codecogs.com/svg.latex?\Large&space; p(A) " title="  " /> est la vraisemblance a priori de la r√©alisation acoustique. Puisque la vraisemblance <img src="https://latex.codecogs.com/svg.latex?\Large&space; p(A) " title="  " /> est la m√™me quelque soit la s√©quence <img src="https://latex.codecogs.com/svg.latex?\Large&space; W " title="  " />. 

### 4.2  Caract√©ristisation du signal 

La repr√©sentation adopt√©e d'un signal de parole consiste en une s√©quence de vecteurs num√©riques  <img src="https://latex.codecogs.com/svg.latex?\Large&space;A = a_1 ... a_T  " title="  " />   o√π chaque vecteur <img src="https://latex.codecogs.com/svg.latex?\Large&space; a_i " title="  " /> repr√©sente quelques millisecondes (typiquement
16 ms) du signal d'entr√©e. De ces  "tranches" de signal, appel√©es trames, des caract√©ristiques relatives √† l'√©nergie et aux gammes de fr√©quences vocales sont extraites. En incluant
les variations du premier, voire du second ordre, de ces param√®tres, les vecteurs de caract
√©ristiques sont typiquement de dimension 40.

![image](https://drive.google.com/uc?export=view&id=1PEHIRd5ayYzZ5IYd0BQmjvDTa_dmvOWi)

Pour notre exemple : Na ngen deef, dalal ak j√†mm si reewu terannga bi. 
![image](https://drive.google.com/uc?export=view&id=1pI-m1qb8SwXx25eVaKRN1CKNQLZJoqS0)


### 4.3  Vocabulaire et lexique phon√©tis√©

Le vocabulaire d√©finit l'ensemble des mots qu'est capable de manipuler le syst√®me de
reconnaissance. En cela, cet ensemble est un √©l√©ment d√©terminant car il restreint les sorties du syst√®me aux seuls mots qu'il contient. Typiquement, dans un syst√®me dit  "√† grand vocabulaire" , le vocabulaire r√©pertorie plusieurs dizaines de milliers de mots. En pratique, pour faire le lien entre mod√©lisations acoustique et linguistique, ce vocabulaire est transform√© en un lexique phon√©tis√© (ou dictionnaire de prononciation, ou dictionnaire phon√©tique) o√π chaque mot est associ√© √† une liste de prononciations possibles.Ces prononciations sont repr√©sent√©es sous la forme de s√©quences de phon√®mes, unit√©s repr√©sentant les sons √©l√©mentaires d'une langue et utilisant un alphabet propre. Il est alors important de bien d√©finir ces r√®gles de phon√©tisation. En guise d'exemple mon nom de famille est **"wade"** et beaucoup prononce **"wad√©"** un autre en wolof **"xar == mouton"** et 
**"xaar == attendre"** qui ne sont pas pareils. 

### 4.4  Mod√®le de langue: cas d'un bi-gramme

Comme nous l'avons bien dit dans les pr√©c√©dentes lignes, le mod√®le de langue permet de calculer la probabilit√© a priori de s√©quences de mots.
Dans le cadre qui nous int√©resse des mod√®les de langue statistiques, cette probabilit√© est
d√©compos√©e en probabilit√©s conditionnelles, o√π la probabilit√© de chaque mot <img src="https://latex.codecogs.com/svg.latex?\Large&space; w_i " title="  " />  de <img src="https://latex.codecogs.com/svg.latex?\Large&space; W " title="  " /> 
est calcul√©e sachant l'historique des mots <img src="https://latex.codecogs.com/svg.latex?\Large&space; w_1,...w_{i-1} " title="  " /> suppos√©s le pr√©c√©der.

Pour un mod√®le de bi-gramme les probabilit√©s conditionnelles sont calcul√©es empiriquement par une technique de maximum de vraisemblance telle que se traduit comme suite:

<img src="https://latex.codecogs.com/svg.latex?\Large&space; p(w_i|w_{i-1}) = \frac{c(w_{i-1}w_i)}{\sum_{w_i}c(w_{i-1}w_i)} " title="  " />

ou <img src="https://latex.codecogs.com/svg.latex?\Large&space; c(w_1,...,w_i) " title="  " /> est le nombre d'occurrences de la s√©quence <img src="https://latex.codecogs.com/svg.latex?\Large&space; w_1,...,w_i " title="  " /> dans un vaste corpus d'apprentissage. 

Prenons un exemple de corpus de trois phrases: 
  - 1) Sama neexalu lijaasa bi kumako jox ? == Qui m'a donn√© le cadeau de mon dipl√¥me ? 
  - 2) Am na sama lijaasa si dara √±u kawee yi  == J'ai eu mon dipl√¥me d'√©tude sup√©rieur 
  - 3) Lane mooy lijaasa si nasaraan == C'est quoi lijaaaa en fran√ßais ? 

Maintenant proc√©dons aux calculs de probabilit√©s pour une phrase prononc√© via un microphone, du genre la probabilit√© de suite de mot :
 

<img src="https://latex.codecogs.com/svg.latex?\Large&space; p(sama \ \ neexalu \ \ lijaasa \ \ bi)  =   " title="  " />

<img src="https://latex.codecogs.com/svg.latex?\Large&space; p(sama |  \alpha) p(neexal | sama) p(lijaasa |  neexalu) p(bi | lijaasa) p(\alpha |  bi )  " title="  " />

o√π <img src="https://latex.codecogs.com/svg.latex?\Large&space; \alpha " title="  " /> est une variable libre.


![image](https://drive.google.com/uc?export=view&id=1K2udnFV7ZX6v91v5gWY-Yzo2HVckjLxa)

     *Remarque 1*: Un N-gramme n'est rien d'autre que la probabilit√© du mot qui suive
     Exemple: Sama neexalu lijaasa bi kumako *jox* 
     P(jox | Sama, neexalu, lijaasa, bi, kumako)
     Uni-gramme : P(jox)
     Bi-gramme : P(jox | kumako)
     Tri-gramme : P(jox | bi kumako)
     etc. 
     
     *Remarque2* Le calcul des probabilit√©s s'effectue en utilisant la formule des probabilit√©s conditionnelles.
      - P(A, B) = P(B|A)P(B)
      - P(A, B, C, D) = P(A)xP(B|A)xP(C|A,B)xP(D|A,B,C)
      - P(Sama, neexalu, lijassa, bi, kumako, jox) =
       P(Sama)xP(neexalu|sama)xP(lijassa|sama neexalu)xP(bi|sama neexalu lijassa)x
       P(kumako|sama neexalu lijassa bi)xP(jox|sama neexalu lijassa bi kumako)
     
    
     
 

### 4.5  Cas des z√©ros et  non vus  

Imaginons maintenant que nous devons pr√©dire une nouvelle phrase prononc√©e par un utilisateur du syst√®me. Si cette derni√®re contienne un nouveau mot ne faisant pas parti de notre corpus de d√©part, la probabilit√© sera sans doute nulle puisque c'est un produit de probabilit√© donn√© par: <img src="https://latex.codecogs.com/svg.latex?\Large&space; p(s) = \prod_{i=1}^{l+1} p(w_{i-1}|w_i) " title="  " /> et que **0 est l'√©l√©ment absorbant de la multiplication (5√©me coll√®ge)** ;). On a alors :

p( sunu bataxalu lijaasa bi) 
= p(sunu \| ‚Ä¢) p(bataxalu \| sunu) p(lijaasa \|  bataxalu) p(bi \| lijaasa) p(‚Ä¢ \|  bi ), 

ce qui nous donne:
![image](https://drive.google.com/uc?export=view&id=1vZb2doqqrIXs0OL8FkM7Lpd4UUKEGCI0)
Donc aucune chacune de pr√©dire cette phrase. 
Nous avons ainsi vu le soucis qu'on peut rencontrer √† faire de la reconnaissance automatique de la parole pour des corpus ne contenant pas assez de **mots uniques**. C'est le cas des **"low resources languages"** c'est-√†-dire les **langues sous dot√©es**. 
C'est un constat fait sur les **langues d'Afrique Subsaharien**, en particulier le wolof. 
Nous pouvons aussi avoir le cas des **z√©ros** qu'on peut d√©finir comme un ou plusieurs mots qui ne sont pas dans notre training set (jeu de donn√©es d'entra√Ænement) et que nous les retrouvons dans notre jeu donn√©es de test lors de **l'√©valuation du mod√®le de langage**.  
Heureusement on peut y rem√©dier dans certains cas. C'est ce que nous verrons dans la section suivante. 

### 4.6  Lissage de Winten-Bell.

Consid√©rons ces deux expressions :  " Je suis "  et " Je supporte " pour un corpus donn√©. Il est plus probable d'avoir "je suis grand " que " je supporte grand ". 
Ainsi le lissage dit de **Winten-Bell**, permet d'√©viter les **probabilit√©s nulles en mod√©lisant la probabilit√© d'observer un N-gramme pour la premi√®re fois**. Yesss voil√† une bonne nouvelle !!!

##### 4.6.1  L'√©quation de Wintten-Bell

<img src="https://latex.codecogs.com/svg.latex?\Large&space; P_{WB}(w_n|w^{n-1}_{n-N+1}) = " title="  " /> 

<img src="https://latex.codecogs.com/svg.latex?\Large&space; \lambda_{w^{n-1}_{n-N+1}}P(w_n|w^{n-1}_{n-N+1}) + (1- \lambda_{w^{n-1}_{n-N+1}})P_{WB}(w_n|w^{n-1}_{n-N+2}) " title="  " />

Pour calculer les param√®tres d'ajustement ùúÜ on se base sur le nombre de mots distincts qui suivent un historique <img src="https://latex.codecogs.com/svg.latex?\Large&space; w^{n-1}_{n-N+1}" title=" " /> tel que:

 <img src="https://latex.codecogs.com/svg.latex?\Large&space; N_{1+}(w^{n-1}_{n-N+1} \alpha) = card[w_n : C(w_n|w^{n-1}_{n-N+1})>0] " title=" " />  
 o√π

 <img src="https://latex.codecogs.com/svg.latex?\Large&space; N_{1+}(w^{n-1}_{n-N+1} \alpha)" title=" " /> 
 est le nombre de N-gramme vus une ou plusieurs fois et qui d√©butent par <img src="https://latex.codecogs.com/svg.latex?\Large&space; w^{n-1}_{n-N+1} " title=" " /> 
 avec <img src="https://latex.codecogs.com/svg.latex?\Large&space; \alpha " title=" " />   une variable libre. 
 
 Pour calculer les 
 <img src="https://latex.codecogs.com/svg.latex?\Large&space; \lambda_{w^{n-1}_{n-N+1}}" title=" " /> 
 sensibles au contexte,  **Witten et Bell** proposent : 

 <img src="https://latex.codecogs.com/svg.latex?\Large&space; 1- \lambda_{w^{n-1}_{n-N+1}} = \frac{N_{1+}(w^{n-1}_{n-N+1} \alpha)}{N_{1+}(w^{n-1}_{n-N+1} \alpha) + C(w_n;w^{n-1}_{n-N+1}) }" title=" " /> 

 Ainsi en faisant une substitution de l'√©quation initiale, *tout en sachant que le diable est dans les d√©tails*,  nous avons alors: 

 <img src="https://latex.codecogs.com/svg.latex?\Large&space; P_{WB}(w_n |  w^{n-1}_{n-N+1}) = \frac{C(w_n;w^{n-1}_{n-N+1}) + N_{1+}(w^{n-1}_{n-N+1} \alpha)P_{WB}(w_n;w^{n-1}_{n-N+2})}{N_{1+}(w^{n-1}_{n-N+1} \alpha) + C(w_n;w^{n-1}_{n-N+1})}" title=" " /> 

 Le lissage de Witten-Bell **accordera ainsi plus de poids √† un N-gramme d√©j√† vus** et sinon, **sera en mesure d'estimer la probabilit√© de rencontrer une nouvelle N-gramme !!**. 
 Ce qui est excellent pour rem√©dier √† ce probl√®me, c'est-√†-dire des cas non vus, qui est un d√©fi majeur des langues sous dot√©s.   
 
 

**Application du lissage de Witten-Bell pour un mod√®le de bi-gramme** ...... en construction 


# Annexe 

**L'alphabet phon√©tique du wolof**
 - les voyelles br√®ves
 
(a) - lal / lit 

(√†) - j√†ng / lire, apprendre

(i) - fit / courage

(o) - for / ramasser

(e) - set / √™tre propre

(√©) - g√©nn / sortir

(u) - tur / nom

(√´) - b√´t / ≈ìil

(√≥) - j√≥g / se lever

 - la voyelle nasale
 
(√•) - s√•s / √™tre tr√®s chaud

 - les voyelles longues.
 
(aa) - baat / mot, cou

(ii) - jiit / scorpion

(oo) - door / d√©buter

(√≥o) - d√≥or / frapper, battre

(ee) - seetu / miroir

(√©e) - r√©ew / pays

(uu) - buur / roi

(√´e) - b√´er / beurre

 - les consonnes orales
 
(b) - benn / un, une

(c) - caabi / cl√©

(d) - dem / partir, aller

(f) - fal / √©lire

(g) - gaal / pirogue

(*) - *aam / menton

(j) - jaan / serpent

(k) - kaw / haut

(l) - lem / plier

(m) - mar / √™tre soif

(n) - naan / boire

(√±) - √±uul / noir

(p) - paaka / couteau

(q) - d√†q / renvoyer

(r) - r√´dd / tracer

(s) - s√´r / pagne

(t) - t√´b / sauter

(w) - w√©et / √™tre solitaire

(x) - xar / couper ; mouton

(y) - yar / √©duquer ; cravache


 - les consonnes nasales
 
(nd) - ndaje / r√©union, rencontre

(ng) - ngoon / apr√®s midi, soir

(nj) - njaay / vente

(nk) - t√†nk / pied

(nq) - x√†nq / bois

(mb) - mburu / pain

(nx) - lenxali / rincer

(nc) - p√©nc / place publique

[Source](http://people.irisa.fr/Gwenole.Lecorve/lectures/ASR.pdf)


