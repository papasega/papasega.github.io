---
layout: post
title: Introduction en Machine Learning 
subtitle: Apprentissage machine 
bigimg: img/posts/MachineLearning_blog.png
share-img: img/ML_intro.png
tags: [IA, Machine Learning, PSW]
comments: true
---

{% highlight python %}
{% endhighlight %}

<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


## Introduction
Le machine learning ou Apprentissage automatique est un mot très en vogue dans le domaine du Big Data.

![image](https://drive.google.com/uc?export=view&id=1Qd5mc3gET31KjjuI1lATMeOsA3-400sM)


Le machine learning est domaine captivant. Issu de nombreuses diciplines comme les statistiques, l'optimisation, l'argorithmique ou le traitement du signale c'est un champ d'études en mutation constante qui s'est imposé dans notre socièté. Déjà utilisé depuis des décennies dans la reconnaissance automatique des caratères ou les filtres anti-spam, il sert maintenant à protéger contre la fraude bancaire, prédire la crimininalité, recommandation des services et produits adptés à nos goûts, reconnaître des visages par exemple avec notre appareil photo etc. Nous allons voir par la suite une définition et des exemples afin de mieux appréhender ce domaine très en mode. 

La science des données (ou data science en anglais) et le machine learning (ou apprentissage
automatique) sont deux mots très en vogue lorsque l’on parle de la révolution Big Data, de prédiction
des comportements ou tout simplement de la transformation numérique des entreprises.
Et comme pour tous les domaines innovants, il est parfois difficile de s’y repérer.
C’est pourquoi, avant de rentrer dans le vif du sujet, nous allons faire un tour rapide du domaine
de la science des données, et en quoi elle est devenue une source de valeur ajoutée pour
les entreprises.
Le premier objectif du data scientist est de produire des méthodes (automatisées, autant que
possible) de tri et d’analyse de données afin d’en extraire des informations utiles.
Pourquoi un data scientist ? Pour trois raisons:
 - L’explosion de la quantité de données produites et collectées par les humains,
 - L’amélioration et l’accessibilité plus grande des algorithmes de traitement des données,
 - L'augmentation exponentielle des capacités de calcul des ordinateurs.
 
En d'autres termes l’objectif étant de récupérer des données de plusieurs sources différentes, et d’en extraire des informations qui vont servir l’entreprise, notamment l’aide à la décision.

En somme la data science est un nouveau domaine de travail, qui augmente les capacités d’analyse classique, afin d’aider les entreprises à prendre des décisions informées. Elle s’appuie pour cela sur des données utiles et ne peut s’appliquer que dans certaines problématiques précises.

Avant d'entrer dans le but du sujet j'ai pensé que ce serait une bonne idée de faire quelques rappels statistiques. Ce document est une introduction plus détaillé sur les statistiques, certains concepts seront illustrés par des codes Python. 

## Qu'est ce que le machine learning ?

Le Machine learning n’est pas nouveau (plusieurs décennies de recherches) mais le concept connaît un engouement ces dernières années qui n’est pas déconnecté de l’essor du Big Data. Mais commençons par le commencement. Arthur Samuel (1959) (un pionnier en matière d’intelligence artificielle, professeur à Stanford ) avait définit le machine learning comme « le domaine d’étude qui donne aux ordinateurs la capacité d’apprendre sans être explicitement programmé ». Cette définition, qui laisse un peu songeur, est quand même vague. Aujourd'hui, Tom Mitchell (1998) en donne une définition un peu plus moderne : « Un programme informatique se dit d’apprendre de l’expérience E par rapport à une catégorie de tâches T et mesure de la performance P, si sa performance à des tâches  T, telle que mesurée par P, s’améliore avec l’expérience E.


Le machine Learning traite des sujets complexes où la programmation traditionnelle trouve ses limites. Construire un programme qui conduit une voiture serait très complexe voire impossible. Cela étant dû aux nombres infinis des cas possibles à traiter… Le Machine learning traite cette problématique différemment. Au lieu de décrire quoi faire, le programme apprendra par lui même comment conduire en “observant” des expérimentations.

     Machine Learning : Donner la possibilité à l’ordinateur d’apprendre sans être programmé

En fonction des données d’expérimentation que prendra l’algorithme d’apprentissage en entrée, il déduira par lui même une hypothèse de fonctionnement.  Il utilisera cette dernière pour de nouveaux cas, et affinera son expérience au fil du temps.

#### Types de problèmes en Machine Learning (ML)

On distingue deux types de problèmes en ML: 
  - Supervised Learning ou Apprentissage Supervisé
  - Unsupervised Learning ou Appentissage non Supervisé.
  
##### Supervised Learning
C’est le type le plus récurrent.  Il s’agit de fournir aux algorithmes d’apprentissages un jeu de données d’apprentissage (Training Set) sous forme de (X, Y) avec X les variables prédictives, et Y le résultat de l’observation. En se basant sur le Training Set, l’algorithme va trouver une fonction mathématique qui permet de transformer (au mieux) X vers Y. En d’autres termes, on l’algorithme va trouver une fonction F tel que : <img src="https://latex.codecogs.com/svg.latex?\Large&space;F(X) = Y " title=" F(X)=Y" />  

     Exemple: 
 
En ayant un Training Set  composé de plusieurs lignes ou chacune décrit les caractéristiques d’une voiture (variables prédictives) : comme le modèle, la couleur, nombre Km parcourus…. et le prix du véhicule (variable Y à prédire). je peux appliquer un algorithme supervisé pour définir une fonction de prédiction de prix d’une voiture. Cette fonction pourra être utilisée pour prédire le prix d’une voiture qui n’était pas dans le Training Set.

##### Unsupervised Learning 
Dans ce type, on va donner à l’algorithme des données, éventuellement non structurées. Et on le laisse trouver une sorte de structure dans nos données. Cela peut être des regroupements de données (Clustering) autrement dit la classification.

     Exemple: 
    
En disposant des données d’achats des internautes dans un  site e-commerce (amazone par exemple), l’algorithme de clustering va trouver les produits qui se vendent le mieux ensemble.

## Modèle linéaire en dimension 1.

L’origine du mot régression vient de Sir Francis Galton. En 1885, travaillant
sur l’hérédité, il chercha à expliquer la taille des fils en fonction de celle des
pères. Il constata que lorsque le père était plus grand que la moyenne,taller
than mediocrity, son fils avait tendance à être plus petit que lui et, a contrario,
que lorsque le père était plus petit que la moyenne,shorter than mediocrity,
son fils avait tendance à être plus grand que lui. Ces résultats l’ont conduit à
considérer sa théorie deregression toward mediocrity.

La base statistique la plus fondamentale des modèles de Machine Learning est la suivante: 
<img src="https://latex.codecogs.com/svg.latex?\Large&space;Y_i = f(X_i) + \epsilon " title=" Y_i = f(X_i) + \epsilon" /> . 

Cette équation simple indique ce qui suit:  

 - nous supposons que nous avons n observations d'un ensemble de données, et nous choisissons sur <img src="https://latex.codecogs.com/svg.latex?\Large&space; i^{eme} " title=" i^{eme}" /> : 
 - <img src="https://latex.codecogs.com/svg.latex?\Large&space;Y_i " title=" Y_i" />   est la sortie de cette observation appelée la cible
 - <img src="https://latex.codecogs.com/svg.latex?\Large&space;X_i " title=" X_i" />   est appelé une caractéristique et est une variable indépendante que nous observons
 - <img src="https://latex.codecogs.com/svg.latex?\Large&space;f" title=" f" />   est le vrai modèle qui établit le lien entre les caractéristiques et la sortie
 - <img src="https://latex.codecogs.com/svg.latex?\Large&space; \epsilon " title=" \epsilon" />   est le bruit du modèle. Les données que nous observons ne se situent généralement pas sur une ligne droite, car il existe des variations de la mesure dans la vie réelle.

Dans cet article nous considérons un cadre assez simple c'est à dire que nous n'avons qu'une seule fonction par observation. Dans la vraie vie, nous avons généralement plusieurs fonctionnalités par observation. Un exemple de ceci pourrait être le suivant: vous travaillez en tant que data scientist dans une banque, vous auriez généralement plusieurs informations sur chaque client: nom, adresse, âge, famille, compte, revenu ect.

# Motivation statistique

Supposons que nous avons <img src="https://latex.codecogs.com/svg.latex?\Large&space;n" title=" n" />  observations <img src="https://latex.codecogs.com/svg.latex?\Large&space;i" title=" i" /> , et pour chaque observation , nous avons une caractéristique <img src="https://latex.codecogs.com/svg.latex?\Large&space;X_i " title=" X_i" /> . Notre rôle est d'identifier le lien entre <img src="https://latex.codecogs.com/svg.latex?\Large&space;Y_i " title=" Y_i" /> et  <img src="https://latex.codecogs.com/svg.latex?\Large&space;X_i " title=" X_i" /> pour pouvoir prédire  <img src="https://latex.codecogs.com/svg.latex?\Large&space;Y_{n+1} " title=" Y_{n+1}" />.\\ Pour construire notre prédiction, nous devons identifier :

   - un modèle, qui est le lien entre <img src="https://latex.codecogs.com/svg.latex?\Large&space;Y_i " title=" Y_i" /> et les entités, représenté ici par la fonction f dans:<img src="https://latex.codecogs.com/svg.latex?\Large&space;Y_i = f(X_i) + \epsilon " title=" Y_i = f(X_i) + \epsilon" />. Dans la vie réelle, ce modèle <img src="https://latex.codecogs.com/svg.latex?\Large&space;f " title=" f" /> est inconnu et est estimé par: <img src="https://latex.codecogs.com/svg.latex?\Large&space;\hat{Y_i} = \hat{f}(X_i)" title="\hat{Y_i} = \hat{f}(X_i)" /> où le chapeau décrit une estimation. Nous essayons d'estimer <img src="https://latex.codecogs.com/svg.latex?\Large&space; \hat{f}" title="\hat{f}" /> qui correspond à la vie réelle <img src="https://latex.codecogs.com/svg.latex?\Large&space;f" title="f" />.
   - nous devons également identifier des paramètres qui aideront notre prédiction à se rapprocher le plus possible des données réelles. Les paramètres sont généralement définis afin de minimiser la distance entre notre modèle et les données, c'est-à-dire qu'ils sont définis de sorte que la dérivée d'une fonction de perte (par exemple: <img src="https://latex.codecogs.com/svg.latex?\Large&space;\sum( \hat{Y_i} - Yi)^2" title="\sum( \hat{Y_i} - Yi)^2" /> ) soit égale à 0.
   
Le vrai modèle et les vrais paramètres dans la vie réelle sont inconnus. Pour cette raison, nous faisons une sélection parmi plusieurs modèles (linéaires ou non linéaires). La sélection du modèle est généralement basée sur une analyse exploratoire des données.


Désormais, nous nous concentrerons sur le modèle le plus élémentaire appelé régression linéaire unidimensionnelle. Ce modèle indique qu'il existe un lien linéaire entre les entités et la variable dépendante.

Le vrai modèle que nous attendons est le suivant: <img src="https://latex.codecogs.com/svg.latex?\Large&space;Y_i = \beta_0 + \beta_1 X_i + \epsilon_i" title="Y_i = \beta_0 + \beta_1 X_i + \epsilon_i" />  :
    - <img src="https://latex.codecogs.com/svg.latex?\Large&space;\beta_0 " title=" \beta_0" /> est appelé l'interception, c'est un terme constant.
    - <img src="https://latex.codecogs.com/svg.latex?\Large&space;\beta_1 " title=" \beta_1" />  est le coefficient associé à <img src="https://latex.codecogs.com/svg.latex?\Large&space;X_i " title=" X_i" />. Il décrit le poids de <img src="https://latex.codecogs.com/svg.latex?\Large&space;X_i " title=" X_i" /> sur la sortie finale.
    - <img src="https://latex.codecogs.com/svg.latex?\Large&space;\epsilon_i " title=" \epsilon_i" />  est appelé résiduel. C'est un terme de bruit blanc qui explique la variabilité des données réelles.
 
 
L'hypothèse sur <img src="https://latex.codecogs.com/svg.latex?\Large&space;\epsilon " title=" \epsilon" /> sont : <img src="https://latex.codecogs.com/svg.latex?\Large&space;\mathbb{E}[\epsilon] = 0  " title=" \mathbb{E}[\epsilon] = 0 " /> , c'est-à-dire une condition de bruit blanc <img src="https://latex.codecogs.com/svg.latex?\Large&space;\epsilon_i \sim iid " title=" \epsilon_i \sim iid" />  pour tout i = 1,…,n, c'est-à-dire une condition [d'homoscédasticité](https://fr.wikipedia.org/wiki/Homosc\%C3\%A9dasticit\%C3\%A9). On parle d'homoscédasticité lorsque la variance des erreurs stochastiques de la régression est la même pour chaque observation i (de 1 à n observations)
 
 
Cependant, les vrais paramètres restent inconnus car ce sont ceux que nous prévoyons d'estimer. Par conséquent, le modèle que nous estimons est le suivant: <img src="https://latex.codecogs.com/svg.latex?\Large&space;\hat{Y_i} =  \hat{\beta_0} +  \hat{\beta_1}X_i " title=" \hat{Y_i} =  \hat{\beta_0} +  \hat{\beta_1}X_i" />. 
 
 
Le mieux pour expliquer ce que ça signifie est de prendre un petit exemple simple. Imaginez que vous voulez savoir si vous payez trop
cher votre loyer. Vous avez récupéré sur un site de location une trentaine de prix des locations disponibles, ainsi que la surface associée :

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import sklearn.linear_model as lm
import math
from scipy import stats

df = pd.read_csv('house.txt', sep=",")
df.head(6)
```

![image](https://drive.google.com/uc?export=view&id=1homURfKq8nXEryATjXsaYB8I7wvKw4HQ)
